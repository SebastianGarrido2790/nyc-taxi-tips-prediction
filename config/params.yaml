Baseline:
  strategy: "mean"

ElasticNet:
  alpha: 0.1
  l1_ratio: 0.5
  random_state: 42

Ridge:
  alpha: 1.0
  random_state: 42

RandomForest:
  n_estimators: 100
  max_depth: 10
  min_samples_split: 5
  random_state: 42

XGBoost:
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  objective: "reg:squarederror"
  random_state: 42

GradientBoosting:
  n_estimators: 100
  learning_rate: 0.1
  max_depth: 3
  random_state: 42

mlflow:
  uri: "https://dagshub.com/SebastianGarrido2790/nyc-taxi-tips-prediction.mlflow"

Training:
  subsample_fraction: 0.2  # Set to a value < 1.0 for faster local training (e.g., 0.1)
  selection_metrics:
    mae: 0.7               # Weight for MAE (lower is better)
    mse: 0.2               # Weight for MSE (lower is better)
    r2: 0.5                # Weight for R2 (higher is better)
    # You can set any metric weight to 0.0 or remove it entirely to ignore it.
    # Where 1.0 is always the "best" performance for that specific metric.
    # The system will automatically normalize and re-weight the remaining metrics.
